package shardkv


import "../shardmaster"
import "../labrpc"
import "../raft"
import "sync"
import "sync/atomic"
import "../labgob"

import "time"

import "bytes"

import "math"

import "log"


type Op struct {
	// Your definitions here.
	// Field names must start with capital letters,
	// otherwise RPC will break.

	// Your definitions here.
	// Field names must start with capital letters,
	// otherwise RPC will break.
	Sequence_Number int // index of op the client submit, i means ith operation 
	// (indicating client has received respone for all ops from 1 to i-1)
	// the sequence number is Gid specific, that means we guarantee sequential execution of operations a client sends to a server

	Client_Serial_Number int64 // serial_number for client who initate this op, used in conjunction with Sequence_Number
	// for duplication detection

	Key   string
	Value string
	Operation   string

	Client_Config_Num int // the config number the client had when sending the request, which is required to ensure
	// the gid server is the server of correct shard ownership the moment request is processed

	Shard_Num int // indicate which shard the key we operate on in get and putAppend belongs to 


	New_Config shardmaster.Config // if action is to update config, this is the config the server cluster tries to update its shards to

	Shard_To_Update Shard // if action is to update a shard, this is the shard this gid server obtained from previous owner

}

type Shard struct {
	Shard_Num int // the index corresponding to current shard
	Config_Num int // the version of config the shard corresponses to
	State int // state of shard, could be garbage, pull, serve, send, see shardkv_global_var for specific definition
	Data map[string]string // the actual state machine of the data
}

type Client struct {
	Received_Sequence_Number int // highest sequence number of op whose response has been received by the client, 
	// so all op with sequence from 1 to Received_Sequence can be deleted since client already has the sequence

	Last_Processed_Sequence_Number int // the sequence number of last operation executed by server,
	// all op with seq_num <= Last_Processed_Sequence_Number should not be executed since they have been executed already  by This kvserver

	Cached_Response map[int] *StoredReply
}

type SnapshotCommand struct {
	LastIncludedIndex int // the index of command for last operation that is executed by the server
	LastIncludedTerm int // the term of command for last operation that is executed by the server

	DbState map[int]map[string]string
	Clients_Info map[int64]*Client //map from client serial number to its state pertaining cached responses
}

type ShardKV struct {
	mu           sync.Mutex
	me           int
	rf           *raft.Raft
	applyCh      chan raft.ApplyMsg
	dead         int32 // set by Kill()
	make_end     func(string) *labrpc.ClientEnd
	gid          int
	masters      []*labrpc.ClientEnd
	maxraftstate int // snapshot if log grows this big

	// Your definitions here.

	mck *shardmaster.Clerk

	clients_Info map[int64]*Client //map from client serial number to its state pertaining cached responses, which only pertains o current server with gid
	//db map[string]string

	db []Shard // the db array where db[i] indicates the shard object for shard i

	lastIncludedIndex int
	lastIncludedTerm int

	operationBuffer []Op // stores the operations whose index is not 
	// one above lastIncludedIndex, indicating that the raft is outdated and the raft should be waiting for snapshot from the leader
	indexBuffer []int // stores index corresponding to operations in operationBuffer
	termBuffer []int // stores term corresponding to operations in operarionBuffer

	configs []shardmaster.Config // the queue that contains config that has been agreed upon by quorum of servers through raft
	
	gid_leader_id map[int]int // leader of each gid group. This is necessary to expedite shard movement, which 
	// requires communication between leader of current gid group and leader of target gid group

	config_Update_Channels []chan int
	// a channel that sends an integer corresponds to version update to shard migration ethod whenever there is 
	// a new config that raft cluster agrees upon
	intermediate_Channels []chan int
	// a chennel that receives an int whenever there is an agreement upon state change of shard during migration
	
}


type StoredReply struct {
	Err string
	Value string
}

func (kv *ShardKV) tryInitSnapShot() {
	_, isLeader := kv.rf.GetState()

	if !isLeader {
		return
	} 

	if kv.maxraftstate == -1 {
		return
	}

	LastIncludedIndex := kv.lastIncludedIndex
	LastIncludedTerm := kv.lastIncludedTerm
	//log.Printf("KvServer %d init snapshot with LastIncludedIndex %d, LastIncludedTerm %d", kv.me, LastIncludedIndex, LastIncludedTerm)

	w := new(bytes.Buffer)
	e := labgob.NewEncoder(w)
	e.Encode(kv.clients_Info)
	e.Encode(kv.db)
	e.Encode(kv.configs)

	SnapShotByte := w.Bytes()

	kv.rf.InitInstallSnapshot(LastIncludedIndex, LastIncludedTerm, SnapShotByte)

	
	return


}


func (kv *ShardKV) Get(args *GetArgs, reply *GetReply) {
	// Your code here.
	kv.mu.Lock()
	defer kv.mu.Unlock()
	key := args.Key

	Client_Serial_Number := args.Client_Serial_Number
	Sequence_Number := args.Sequence_Number

	Client_Config_Num := args.Client_Config_Num

	client_Info_This, ok := kv.clients_Info[Client_Serial_Number]
	if !ok {
		// means this leader is the first leader that has received request from this client
		// and This kvserver has not processed any request for this client and no other server has
		// (it others do, )
		client_To_Add := Client{}
		client_To_Add.Received_Sequence_Number = Sequence_Number - 1 //this client must have received all
		// requests before sequence number or it will not fire this request
		client_To_Add.Last_Processed_Sequence_Number = default_sentinel_index // well This kvserver has
		// not execute any operation on this client yet, so we need to wait raft send command
		// so that we can eventually apply operations until the server is at least up to date 
		// as Received_Sequence_Number
		client_To_Add.Cached_Response = make(map[int]*StoredReply)
		// save all responses from Received_Sequence_Number + 1 (since we are not sure if client has received
		// previous response or not, we don't delete until future rpc indicate we can)
		kv.clients_Info[Client_Serial_Number] = &client_To_Add
		client_Info_This = kv.clients_Info[Client_Serial_Number]

	} else {
		// we have this client on file, we can simply delete all requests with 
		// sequence number < sequence number of current request
		for seq_Num, _ := range client_Info_This.Cached_Response {
			if seq_Num < Sequence_Number {
				delete(client_Info_This.Cached_Response, seq_Num)
			}
		}
		// we know all requests up to Sequence_Number - 1 has been received by the client so we need to update Received sequence number as well
		client_Info_This.Received_Sequence_Number = int(math.Max(float64(client_Info_This.Received_Sequence_Number), float64(Sequence_Number - 1)))
		// due to asychronous network, it is possible that the older request arrives This kvserver as result of re routing, but This kvserver already 
		// receives snapshot from previous leader that has handled this request
	}
	

	//log.Printf("This kvserver %d has received Get request with key %s and serial number %d from clerk %d", kv.me, key, Sequence_Number, Client_Serial_Number)

	if kv.killed() {
		reply.Err = ErrServerKilled
		//log.Printf("This kvserver %d has been killed", kv.me)
		return
	} 

	// removed reply to previous rpc already finished

	term, isLeader, currentLeaderId, serverRole := kv.rf.GetStateWTF()

	if !isLeader {
		//log.Printf("This kvserver %d (term %d) has received Get request with key %s and serial number %d but is not leader, re route to leader %d of term %d", kv.me, term, key, Sequence_Number, currentLeaderId, term)
		reply.Err = ErrWrongLeader
		reply.CurrentLeaderId = currentLeaderId
		reply.CurrentLeaderTerm = term
		reply.ServerRole = serverRole
		return
	} else {

		Client_Received_Sequence_Number := client_Info_This.Received_Sequence_Number
		Client_Last_Processed_Sequence_Number := client_Info_This.Last_Processed_Sequence_Number

		if Sequence_Number <= Client_Received_Sequence_Number {
			// dude the client has already received reply, so that reply is just staled and we don't need to do 
			// anything about it
			return
		} else if Sequence_Number <= Client_Last_Processed_Sequence_Number {
			// good, that means cached reply is still in the dictionary
			cachedReply := client_Info_This.Cached_Response[Sequence_Number]

			reply.Err = cachedReply.Err
			reply.Value = cachedReply.Value

			reply.CurrentLeaderId = kv.me
			reply.CurrentLeaderTerm = term

			//log.Printf("This kvserver %d (term %d) has cached result for Get request with key %s, client: %d, seq_num: %d", kv.me, term, key, Client_Serial_Number, Sequence_Number)
			
			return
		} else {
	
			opToRaft := Op{}

			opToRaft.Sequence_Number = Sequence_Number
			opToRaft.Client_Serial_Number = Client_Serial_Number

			opToRaft.Key = key
			opToRaft.Operation = "Get"

			opToRaft.Client_Config_Num = Client_Config_Num

			//_, _, isLeader := kv.rf.Start(opToRaft)
	
			currentLeaderId, index, term, isLeader := kv.rf.StartQuick(opToRaft)

	
			if index == invalid_index {
				reply.Err = ErrServerKilled
				return
			}
			if !isLeader {
				//log.Printf("This kvserver %d (term %d) has cached result for Get request with key %s and serial number %d but is not leader, re route to leader %d of term %d", kv.me, term, key, Sequence_Number, currentLeaderId, term)
				reply.Err = ErrWrongLeader
				reply.CurrentLeaderId, reply.CurrentLeaderTerm = currentLeaderId, term 
				return
			} else {
				/*if kv.maxraftstate != -1 {
				
					snapShotSize := kv.rf.GetRaftStateSize()
				
					if snapShotSize >= kv.maxraftstate {
						//log.Printf("kvserver %d make snapshot in Get with LastIncludeIndex %d and LastIncludeTerm %d", kv.me, kv.LastIncludedIndex, kv.LastIncludedTerm)
						kv.tryInitSnapShot()
					}
				}*/
				//log.Printf("This kvserver %d (term %d) does not have cached result for Get request with key %s and serial number %d and is a  leader, now enqueue", kv.me, term, key, Sequence_Number)
				kv.mu.Unlock()
			}
			
			for {
				//log.Printf("Kvserver get before lock")
				kv.mu.Lock()
				//log.Printf("Kvserver %d (term %d) get wtf", kv.me, term)
				//log.Printf("Kvserver %d (term %d) get locked", kv.me, term)
				if kv.killed() {
					//log.Printf("This kvserver %d (term %d) has been killed", kv.me, term)
					reply.Err = ErrServerKilled
					return
				} 
				//log.Printf("Kvserver %d (term %d) Get GetStateWtf init", kv.me, term)

				term, isLeader, currentLeaderId, serverRole = kv.rf.GetStateWTF()
		
				//log.Printf("Kvserver %d (term %d) Get GetStateWtf finished", kv.me, term)
				if !isLeader {
					//log.Printf("This kvserver %d (term %d) has received Get request with key %s and serial number %d but is not leader, re route to leader %d of term %d", kv.me, term, key, Sequence_Number, currentLeaderId, term)
				
					reply.Err = ErrWrongLeader
					reply.CurrentLeaderId = currentLeaderId
					reply.CurrentLeaderTerm = term
					reply.ServerRole = serverRole
					return
				} else {
					client_Info_This = kv.clients_Info[Client_Serial_Number]

					Client_Received_Sequence_Number = client_Info_This.Received_Sequence_Number
					Client_Last_Processed_Sequence_Number = client_Info_This.Last_Processed_Sequence_Number
					//log.Printf("Kvserver %d (term %d), for client %d, Get task with sequence number %d, Client_Received_Sequence_Number %d, Client_Last_Processed_Sequence_Number %d", kv.me, term, Client_Serial_Number, Sequence_Number, Client_Received_Sequence_Number, Client_Last_Processed_Sequence_Number)
					if Sequence_Number <= Client_Received_Sequence_Number {
						// dude the client has already received reply, so that reply is just staled and we don't need to do 
						// anything about it
						//log.Printf("Kvserver %d (term %d) wtf2", kv.me, term)
						return
					} else if Sequence_Number <= Client_Last_Processed_Sequence_Number {
						// good, that means cached reply is still in the dictionary
						cachedReply := client_Info_This.Cached_Response[Sequence_Number]
			
						reply.Err = cachedReply.Err
						reply.Value = cachedReply.Value
			
						reply.CurrentLeaderId = kv.me
						reply.CurrentLeaderTerm = term
			
						//log.Printf("This kvserver %d (term %d) has cached result for Get request with key %s, client: %d, seq_num: %d", kv.me, term, key, Client_Serial_Number, Sequence_Number)
						
						return
					} else {
						//log.Printf("This kvserver %d (term %d) does not have cached result for Get request with key %s, client: %d, seq_num: %d, keep waiting...", kv.me, term, key, Client_Serial_Number, Sequence_Number)
						//log.Printf("Kvserver %d (term %d) get Unlocked", kv.me, term)
						kv.mu.Unlock()
					}
				}
				
				time.Sleep(time.Duration(kvserver_loop_wait_time_millisecond) * time.Millisecond)
			}
		}
	}
}

func (kv *ShardKV) PutAppend(args *PutAppendArgs, reply *PutAppendReply) {
	// Your code here.
	kv.mu.Lock()
	defer kv.mu.Unlock()
	key := args.Key
	value := args.Value
	op := args.Op

	Client_Serial_Number := args.Client_Serial_Number
	Sequence_Number := args.Sequence_Number

	Client_Config_Num := args.Client_Config_Num

	client_Info_This, ok := kv.clients_Info[Client_Serial_Number]
	if !ok {
		// means this leader is the first leader that has received request from this client
		// and This kvserver has not processed any request for this client and no other server has
		// (it others do, )
		client_To_Add := Client{}
		client_To_Add.Received_Sequence_Number = Sequence_Number - 1 //this client must have received all
		// requests before sequence number or it will not fire this request
		client_To_Add.Last_Processed_Sequence_Number = default_sentinel_index // well This kvserver has
		// not execute any operation on this client yet, so we need to wait raft send command
		// so that we can eventually apply operations until the server is at least up to date 
		// as Received_Sequence_Number
		client_To_Add.Cached_Response = make(map[int]*StoredReply)
		// save all responses from Received_Sequence_Number + 1 (since we are not sure if client has received
		// previous response or not, we don't delete until future rpc indicate we can)
		kv.clients_Info[Client_Serial_Number] = &client_To_Add
		client_Info_This = kv.clients_Info[Client_Serial_Number]

	} else {
		// we have this client on file, we can simply delete all requests with 
		// sequence number < sequence number of current request

		//log.Printf("number of cached response for client %d before deletion is %d", Client_Serial_Number, len(client_Info_This.Cached_Response))

		for seq_Num, _ := range client_Info_This.Cached_Response {
			if seq_Num < Sequence_Number {
				delete(client_Info_This.Cached_Response, seq_Num)
			}
		}
		//log.Printf("number of cached response for client %d after deletion is %d", Client_Serial_Number, len(client_Info_This.Cached_Response))
		// we know all requests up to Sequence_Number - 1 has been received by the client so we need to update Received sequence number as well
		client_Info_This.Received_Sequence_Number = int(math.Max(float64(client_Info_This.Received_Sequence_Number), float64(Sequence_Number - 1)))
		// due to asychronous network, it is possible that the older request arrives This kvserver as result of re routing, but This kvserver already 
		// receives snapshot from previous leader that has handled this request
	}
	

	//log.Printf("This kvserver %d has received Get request with key %s and serial number %d from clerk %d", kv.me, key, Sequence_Number, Client_Serial_Number)

	if kv.killed() {
		reply.Err = ErrServerKilled
		//log.Printf("This kvserver %d has been killed", kv.me)
		return
	} 

	// removed reply to previous rpc already finished
	term, isLeader, currentLeaderId, serverRole := kv.rf.GetStateWTF()

	if !isLeader {
		//log.Printf("This kvserver %d (term %d) has received Get request with key %s and serial number %d but is not leader, re route to leader %d of term %d", kv.me, term, key, Sequence_Number, currentLeaderId, term)
		reply.Err = ErrWrongLeader
		reply.CurrentLeaderId = currentLeaderId
		reply.CurrentLeaderTerm = term
		reply.ServerRole = serverRole
		return
	} else {
		Client_Received_Sequence_Number := client_Info_This.Received_Sequence_Number
		Client_Last_Processed_Sequence_Number := client_Info_This.Last_Processed_Sequence_Number

		if Sequence_Number <= Client_Received_Sequence_Number {
			// dude the client has already received reply, so that reply is just staled and we don't need to do 
			// anything about it
			return
		} else if Sequence_Number <= Client_Last_Processed_Sequence_Number {
			// good, that means cached reply is still in the dictionary
			cachedReply := client_Info_This.Cached_Response[Sequence_Number]

			reply.Err = cachedReply.Err

			reply.CurrentLeaderId = kv.me
			reply.CurrentLeaderTerm = term

			//log.Printf("This kvserver %d (term %d) has cached result for Get request with key %s, client: %d, seq_num: %d", kv.me, term, key, Client_Serial_Number, Sequence_Number)
			
			return
		} else {
	
			opToRaft := Op{}

			opToRaft.Sequence_Number = Sequence_Number
			opToRaft.Client_Serial_Number = Client_Serial_Number

			opToRaft.Key = key
			opToRaft.Value = value
			opToRaft.Operation = op

			opToRaft.Client_Config_Num = Client_Config_Num

			//_, _, isLeader := kv.rf.Start(opToRaft)
	
			currentLeaderId, index, term, isLeader := kv.rf.StartQuick(opToRaft)
	

			if index == invalid_index {
				reply.Err = ErrServerKilled
				return
			}
			if !isLeader {
				//log.Printf("This kvserver %d (term %d) has cached result for Get request with key %s and serial number %d but is not leader, re route to leader %d of term %d", kv.me, term, key, Sequence_Number, currentLeaderId, term)
				reply.Err = ErrWrongLeader
				reply.CurrentLeaderId, reply.CurrentLeaderTerm = currentLeaderId, term
				return
			} else {
				/*if kv.maxraftstate != -1 {
				
					snapShotSize := kv.rf.GetRaftStateSize()
				
					if snapShotSize >= kv.maxraftstate {
						//log.Printf("kvserver %d make snapshot in PutAppend with LastIncludeIndex %d and LastIncludeTerm %d", kv.me, kv.LastIncludedIndex, kv.LastIncludedTerm)
						kv.tryInitSnapShot()
					}
				}*/
				//log.Printf("This kvserver %d (term %d) does not have cached result for Get request with key %s and serial number %d but is not leader, now enqueue", kv.me, term, key, Sequence_Number)
				kv.mu.Unlock()
			}
			
			for {

				//log.Printf("Kvserver before lock")
				kv.mu.Lock()
				//log.Printf("Kvserver %d putappend locked ", kv.me)
				if kv.killed() {
					//log.Printf("This kvserver %d has been killed", kv.me)
					reply.Err = ErrServerKilled
					return
				} 
				//log.Printf("Kvserver %d putappend GetStateWtf init", kv.me)

			
				term, isLeader, currentLeaderId, serverRole = kv.rf.GetStateWTF()
			

				//log.Printf("Kvserver %d (term %d) putappend GetStateWtf finished", kv.me, term)
				if !isLeader {
					//log.Printf("This kvserver %d (term %d) has received Get request with key %s and serial number %d but is not leader, re route to leader %d of term %d", kv.me, term, key, Sequence_Number, currentLeaderId, term)
		
					reply.Err = ErrWrongLeader
					reply.CurrentLeaderId = currentLeaderId
					reply.CurrentLeaderTerm = term
					reply.ServerRole = serverRole
					return
				} else {
					client_Info_This = kv.clients_Info[Client_Serial_Number]

					Client_Received_Sequence_Number = client_Info_This.Received_Sequence_Number
					Client_Last_Processed_Sequence_Number = client_Info_This.Last_Processed_Sequence_Number

					//log.Printf("Kvserver %d (term %d), for client %d, putappend task with sequence number %d, Client_Received_Sequence_Number %d, Client_Last_Processed_Sequence_Number %d", kv.me, term, Client_Serial_Number, Sequence_Number, Client_Received_Sequence_Number, Client_Last_Processed_Sequence_Number)

					if Sequence_Number <= Client_Received_Sequence_Number {
						// dude the client has already received reply, so that reply is just staled and we don't need to do 
						// anything about it
						//log.Printf("Kvserver %d (term %d) wtf2", kv.me, term)
						return
					} else if Sequence_Number <= Client_Last_Processed_Sequence_Number {
						// good, that means cached reply is still in the dictionary
						cachedReply := client_Info_This.Cached_Response[Sequence_Number]
			
						reply.Err = cachedReply.Err
			
						reply.CurrentLeaderId = kv.me
						reply.CurrentLeaderTerm = term
			
						//log.Printf("This kvserver %d (term %d) has cached result for Get request with key %s, client: %d, seq_num: %d", kv.me, term, key, Client_Serial_Number, Sequence_Number)
						
						return
					} else {
						//log.Printf("This kvserver %d (term %d) does not cached result for Get request with key %s, client: %d, seq_num: %d, keep waiting...", kv.me, term, key, Client_Serial_Number, Sequence_Number)
						//log.Printf("Kvserver %d (term %d) putappend Unlocked", kv.me, term)
						kv.mu.Unlock()
					}
				}
				
				time.Sleep(time.Duration(kvserver_loop_wait_time_millisecond) * time.Millisecond)
			}
		}
	}
}

//
// the tester calls Kill() when a ShardKV instance won't
// be needed again. for your convenience, we supply
// code to set rf.dead (without needing a lock),
// and a killed() method to test rf.dead in
// long-running loops. you can also add your own
// code to Kill(). you're not required to do anything
// about this, but it may be convenient (for example)
// to suppress debug output from a Kill()ed instance.
//
func (kv *ShardKV) Kill() {
	atomic.StoreInt32(&kv.dead, 1)
	kv.rf.Kill()
	// Your code here, if desired.
}

func (kv *ShardKV) killed() bool {
	z := atomic.LoadInt32(&kv.dead)
	return z == 1
}
func (kv *ShardKV) emptyOperationBuffer() {
	if (len(kv.operationBuffer) == 0) {
		//nothing in the buffer...
		return
	} 
	opBufferLowerBound := kv.indexBuffer[0]
	opBufferUpperBound := kv.indexBuffer[len(kv.indexBuffer) - 1]
	if (opBufferUpperBound <= kv.lastIncludedIndex) {
		kv.operationBuffer = make([]Op, 0)
		kv.indexBuffer = make([]int, 0)
		kv.termBuffer = make([]int, 0)

		//everything in the buffer has already been applied...
		return
	}

	if (opBufferLowerBound > kv.lastIncludedIndex + 1) {
		//there is a gap between current state machine index and opBuffer index, so we wait for snapshot to fill the gap
		//log.Printf("Kvserver %d, opbufferLowerBound %d, opbufferUpperBound %d, lastIncludeIndex %d, there is a gap, wait for snapshot to fill the gap", kv.me, opBufferLowerBound, opBufferUpperBound, kv.lastIncludedIndex)
		return
	}
	for i := 0; i < len(kv.operationBuffer); i++ {
		commandIndex := kv.indexBuffer[i]
		commandTerm := kv.termBuffer[i]
		if (commandIndex == kv.lastIncludedIndex + 1) {
			operation := kv.operationBuffer[i]
			//log.Printf("Kvserver %d, having LastIncludeIndex %d, applies operation with commandIndex %d, commandTerm %d from emptyOperationBuffer", kv.me, kv.lastIncludedIndex, commandIndex, commandTerm)
			kv.applyOperation(operation)
			kv.lastIncludedIndex = commandIndex
			kv.lastIncludedTerm = commandTerm
		} else {
			//log.Printf("Kvserver %d, having LastIncludeIndex %d, cannot apply operation with commandIndex %d, commandTerm %d from emptyOperationBuffer", kv.me, kv.lastIncludedIndex, commandIndex, commandTerm)
		}
	}
	kv.operationBuffer = make([]Op, 0)
	kv.indexBuffer = make([]int, 0)
	kv.termBuffer = make([]int, 0)
	return
}

func copyShard(shardToCopy Shard) Shard{
	shardToReturn := Shard{}
	shardToReturn.Shard_Num = shardToCopy.Shard_Num
	shardToReturn.Config_Num = shardToCopy.Config_Num
	shardToReturn.State = shardToCopy.State
	shardToReturn.Data = make(map[string]string)

	for key, value := range shardToCopy.Data {
		shardToReturn.Data[key] = value
	}

	return shardToReturn
}

func copyConfig(configToCopy shardmaster.Config) shardmaster.Config{
	configToReturn := shardmaster.Config{}
	configToReturn.Num = configToCopy.Num
	configToReturn.Shards = configToCopy.Shards
	configToReturn.Groups = make(map[int][]string)

	for gid, group := range configToCopy.Groups {
		configToReturn.Groups[gid] = group
	}

	return configToReturn

}

func updateSucceed(currentShard Shard, shardToUpdate Shard) bool{
	if currentShard.Config_Num > shardToUpdate.Config_Num {
		return true
	}
	if currentShard.Config_Num == shardToUpdate.Config_Num {
		if currentShard.State == shardToUpdate.State {
			return true
		} else {
			if (currentShard.State == Serving && shardToUpdate.State == Pulling) ||
			(currentShard.State == Garbage && shardToUpdate.State == Sending) {
				return true
			}
		}
	}
	return false
}

func (kv *ShardKV) syncShardMessageUntilCommit(shardToUpdate Shard) {

	counter := 0
	for {
		kv.mu.Lock()

		if updateSucceed(copyShard(kv.db[shardToUpdate.Shard_Num]), copyShard(shardToUpdate)) {
			kv.mu.Unlock()
			return
		} else {
			if (counter == shard_update_agreement_interval_millisecond) {
				_, isLeader := kv.rf.GetState()
				if (isLeader){
					updateShardOp := Op{}
					updateShardOp.Operation = "Update_Shard"
					updateShardOp.Shard_To_Update = copyShard(shardToUpdate)
					_, index, _, _ := kv.rf.StartQuick(updateShardOp)
					log.Printf("On Server %d of gid %d, Shard %d not finished migrate to version %d with state %s in last %d milliseconds, restart aggrement at index %d", kv.me, kv.gid, shardToUpdate.Shard_Num, shardToUpdate.Config_Num,stateIntToString(shardToUpdate.State), shard_update_agreement_interval_millisecond, index)

				}
				counter = 0

			}
			
			kv.mu.Unlock()
			
		}
		counter = counter + shard_update_check_interval_millisecond

		time.Sleep(time.Duration(shard_update_check_interval_millisecond) * time.Millisecond)
	}
}

func (kv *ShardKV) garbageToGarbage(shardNum int, newVersionNum int) {
	kv.mu.Lock()
	log.Printf("On Server %d of gid %d, shard %d starts migrating from Garbage in version %d to Garbage in version %d", kv.me, kv.gid, shardNum, newVersionNum - 1, newVersionNum)
	shardToUpdate := copyShard(kv.db[shardNum])
	shardToUpdate.Config_Num = newVersionNum

	kv.mu.Unlock()

	kv.syncShardMessageUntilCommit(shardToUpdate)

	//<- intermediate_Channel

	log.Printf("On Server %d of gid %d, shard %d finishes migrating from Garbage in version %d to Garbage in version %d", kv.me, kv.gid, shardNum, newVersionNum - 1, newVersionNum)
}

func (kv *ShardKV) sendAckSignal(shardAcked int, num_Target int, previousOwnerGroup []string, previousOwnerGid int) {
	args := AckShardArgs{}
	args.ShardAcked = shardAcked
	args.Num_Target = num_Target

	log.Printf("Server %d of gid %d starts sending ack signal for shard %d to group with gid %d for migration from version %d to version %d", kv.me, kv.gid, shardAcked, previousOwnerGid, num_Target - 1, num_Target)

	for {
		for si := 0; si < len(previousOwnerGroup); si++ {
			srv := kv.make_end(previousOwnerGroup[si])
			var reply AckShardReply
			ok := srv.Call("ShardKV.AckShard", &args, &reply)
			if ok && (reply.Err == OK) {
				return
			} else if ok && (reply.Err == ErrMigrationInconsistent) {
				// the other server is not up to date and cannot serve the data
				time.Sleep(time.Duration(kvserver_loop_wait_time_millisecond) * time.Millisecond)
				// break inner loop and retry after wait for 5ms
				// but, this should never happen... like
				// we won't send ack without acquiring the data from target server and start serving 
				// so just a precaution...
				break
			} else if ok && (reply.Err == ErrGarbage) {
				// dude the majority of server has already started serving and 
				// the previous owner already marked the data garbage...
				return
			} else if ok && (reply.Err == ErrWrongGroup){
				// must be ErrWrongLeader
				// continue the inner loop to try another server
				continue
			} else {
				//not receive reply possibly due to network connection issue, so we just try another server
				continue
			}
		}
	}


}



func (kv *ShardKV) garbageToServing(shardNum int, newVersionNum int) {

	kv.mu.Lock()
	shardToUpdate := copyShard(kv.db[shardNum])
	shardToUpdate.Config_Num = newVersionNum
	shardToUpdate.State = Pulling
	log.Printf("On Server %d of gid %d, shard %d starts migrating from Garbage in version %d to Serving in version %d", kv.me, kv.gid, shardNum, newVersionNum - 1, newVersionNum)

	kv.mu.Unlock()

	kv.syncShardMessageUntilCommit(shardToUpdate)

	//<- intermediate_Channel
	log.Printf("On Server %d of gid %d, shard %d finished Pulling agreement migrating from Garbage in version %d to Serving in version %d", kv.me, kv.gid, shardNum, newVersionNum - 1, newVersionNum)
	
	kv.mu.Lock()
	previousOwnerGid := kv.configs[newVersionNum - 1].Shards[shardNum]
	previousOwnerGroup := kv.configs[newVersionNum - 1].Groups[previousOwnerGid]

	args := FetchShardArgs{}
	args.ShardRequested = shardNum
	args.Num_Target = newVersionNum

	replyToUse := FetchShardReply{}
	log.Printf("On Server %d of gid %d, shard %d starts fetching data from previous owner %d for migrating from Garbage in version %d to Serving in version %d", kv.me, kv.gid, shardNum, previousOwnerGid, newVersionNum - 1, newVersionNum)
	kv.mu.Unlock()

	outerLoop:
	for {
		for si := 0; si < len(previousOwnerGroup); si++ {
			srv := kv.make_end(previousOwnerGroup[si])
			var reply FetchShardReply
			ok := srv.Call("ShardKV.FetchShard", &args, &reply)
			if ok && (reply.Err == OK) {
				replyToUse.Err = reply.Err
				replyToUse.Data = make(map[string]string)
				for key, value := range reply.Data {
					replyToUse.Data[key] = value
				}
				break outerLoop
			} else if ok && (reply.Err == ErrMigrationInconsistent) {
				// the other server is not up to date and cannot serve the data
				time.Sleep(time.Duration(kvserver_loop_wait_time_millisecond) * time.Millisecond)
				// break inner loop and retry after wait for 5ms
				break
			} else if ok && (reply.Err == ErrGarbage) {
				// dude the majority of server has already started serving and 
				// the previous owner already marked the data garbage...
				replyToUse.Err = reply.Err
				break outerLoop
			} else if ok && (reply.Err == ErrWrongLeader){
				// must be ErrWrongLeader
				// continue the inner loop to try another server
				continue
			} else {
				//not receive reply possibly due to network connection issue, so we just try another server
				continue
			}
		}
	}

	if replyToUse.Err == ErrGarbage {
		log.Printf("On Server %d of gid %d, shard %d has already been marked Garbage from previous owner %d for migrating from Garbage in version %d to Serving in version %d", kv.me, kv.gid, shardNum, previousOwnerGid, newVersionNum - 1, newVersionNum)
		// meaning we should already have served the data
		// and the previous owner has already marked garbage
		return
	} else {
		shardToUpdate := Shard{}
		shardToUpdate.Shard_Num = shardNum
		shardToUpdate.Config_Num = newVersionNum
		shardToUpdate.State = Serving
		shardToUpdate.Data = replyToUse.Data
		log.Printf("On Server %d of gid %d, shard %d successfully fetched data from previous owner %d for migrating from Garbage in version %d to Serving in version %d, now starts aggrement", kv.me, kv.gid, shardNum, previousOwnerGid, newVersionNum - 1, newVersionNum)

		kv.syncShardMessageUntilCommit(shardToUpdate)
	
		//<- intermediate_Channel

		log.Printf("On Server %d of gid %d, shard %d finished migrating from Garbage in version %d to Serving in version %d", kv.me, kv.gid, shardNum, newVersionNum - 1, newVersionNum)
		go kv.sendAckSignal(shardNum, newVersionNum, previousOwnerGroup, previousOwnerGid)
		
		return
	}
	
}

func (kv *ShardKV) servingToServing(shardNum int, newVersionNum int) {
	kv.mu.Lock()
	shardToUpdate := copyShard(kv.db[shardNum])
	shardToUpdate.Config_Num = newVersionNum
	log.Printf("On Server %d of gid %d, shard %d starts migrating from Serving in version %d to Serving in version %d", kv.me, kv.gid, shardNum, newVersionNum - 1, newVersionNum)
	

	kv.mu.Unlock()

	kv.syncShardMessageUntilCommit(shardToUpdate)
	

	//<- intermediate_Channel

	log.Printf("On Server %d of gid %d, shard %d finished migrating from Serving in version %d to Serving in version %d", kv.me, kv.gid, shardNum, newVersionNum - 1, newVersionNum)
	
}

func (kv *ShardKV) servingToGarbage(shardNum int, newVersionNum int) {
	kv.mu.Lock()
	shardToUpdate := copyShard(kv.db[shardNum])
	shardToUpdate.Config_Num = newVersionNum
	shardToUpdate.State = Sending

	log.Printf("On Server %d of gid %d, shard %d start migrating from Serving in version %d to Garbage in version %d", kv.me, kv.gid, shardNum, newVersionNum - 1, newVersionNum)

	kv.mu.Unlock()

	kv.syncShardMessageUntilCommit(shardToUpdate)

	//<- intermediate_Channel
	// now that the cluster has reached agreement on sending, now just wait for 
	// agreement on garbaging
	// which is determined by reception of ack rpc from the current owner of the shard

	//<- intermediate_Channel
	shardToUpdate.State = Garbage
	for {
		kv.mu.Lock()
		if updateSucceed(copyShard(kv.db[shardToUpdate.Shard_Num]), copyShard(shardToUpdate)) {
			kv.mu.Unlock()
			break
		} else {
			kv.mu.Unlock()
			time.Sleep(time.Duration(kvserver_loop_wait_time_millisecond) * time.Millisecond)
			
		}
	}
	log.Printf("On Server %d of gid %d, shard %d finished migrating from Serving in version %d to Garbage in version %d", kv.me, kv.gid, shardNum, newVersionNum - 1, newVersionNum)

	return
}

func (kv *ShardKV) migrateShard(shardNum int, config_Update_Channels chan int) {

	for newVersionNum := range config_Update_Channels{
		if newVersionNum == 1 {
			shardToUpdate := Shard{}
			shardToUpdate.Shard_Num = shardNum
			shardToUpdate.Config_Num = newVersionNum
			shardToUpdate.Data = make(map[string]string)

			kv.mu.Lock()
			if kv.configs[newVersionNum].Shards[shardNum] == kv.gid {		
				shardToUpdate.State = Serving
				log.Printf("On Server %d of gid %d, Shard %d will migrate to Serving in version %d", kv.me, kv.gid, shardNum, newVersionNum)
			} else {
				shardToUpdate.State = Garbage
				log.Printf("On Server %d of gid %d, Shard %d will migrate to Garbage in version %d", kv.me, kv.gid, shardNum, newVersionNum)
			}

			kv.mu.Unlock()

			kv.syncShardMessageUntilCommit(shardToUpdate)

			//<- intermediate_Channel
			log.Printf("On Server %d of gid %d, Shard %d finished migrate to version %d upon exiting the loop", kv.me, kv.gid, shardNum, newVersionNum)

		} else {
			kv.mu.Lock()
			if kv.configs[newVersionNum - 1].Shards[shardNum] != kv.gid && kv.configs[newVersionNum].Shards[shardNum] != kv.gid {
				kv.mu.Unlock()
				kv.garbageToGarbage(shardNum, newVersionNum)
			} else if (kv.configs[newVersionNum - 1].Shards[shardNum] != kv.gid && kv.configs[newVersionNum].Shards[shardNum] == kv.gid) {
				kv.mu.Unlock()
				kv.garbageToServing(shardNum, newVersionNum)
			} else if (kv.configs[newVersionNum - 1].Shards[shardNum] == kv.gid && kv.configs[newVersionNum].Shards[shardNum] == kv.gid) {
				kv.mu.Unlock()
				kv.servingToServing(shardNum, newVersionNum)
			} else {
				kv.mu.Unlock()
				kv.servingToGarbage(shardNum, newVersionNum)
			}
		}
	}
}

func (kv *ShardKV) updateConfig(newConfig shardmaster.Config) {

	if newConfig.Num != len(kv.configs) {
		return
	}
	newVersionNum := newConfig.Num 

	kv.configs = append(kv.configs, newConfig)
	log.Printf("Server %d of gid %d Finished agreement on config %d", kv.me, kv.gid, newVersionNum)
	kv.mu.Unlock()
	

	for i := 0; i < shardmaster.NShards; i++ {
		kv.config_Update_Channels[i] <- newVersionNum
	}

	kv.mu.Lock()

	return
}

func stateIntToString(state int) string{
	if state == Garbage {
		return "Garbage"
	}

	if state == Pulling {
		return "Pulling"
	}

	if state == Serving {
		return "Serving"
	}

	if state == Sending {
		return "Sending"
	}
	return ""
}


func (kv *ShardKV) updateShard(updatedShard Shard) {

	shardNum := updatedShard.Shard_Num

	configNum := updatedShard.Config_Num

	previousShard := kv.db[shardNum]


	log.Printf("On server %d of gid %d, shard %d, from state %s at config %d to state %s at config %d", kv.me, kv.gid, shardNum, stateIntToString(previousShard.State), previousShard.Config_Num, stateIntToString(updatedShard.State), updatedShard.Config_Num)

	if updatedShard.State == Garbage {
		if (previousShard.State == Garbage && previousShard.Config_Num == updatedShard.Config_Num - 1) || 
		(previousShard.State == Sending && previousShard.Config_Num == updatedShard.Config_Num) {
			kv.db[shardNum] = updatedShard
			log.Printf("On server %d of gid %d, successfully update shard %d to state %s at config num %d", kv.me, kv.gid, shardNum, "Garbage", configNum)
			//kv.mu.Unlock()
			//kv.intermediate_Channels[shardNum] <- configNum
			//kv.mu.Lock()
		}
	} else if updatedShard.State == Pulling {
		if (previousShard.State == Garbage && previousShard.Config_Num == updatedShard.Config_Num - 1) {
			kv.db[shardNum] = updatedShard
			log.Printf("On server %d of gid %d, successfully update shard %d to state %s at config num %d", kv.me, kv.gid, shardNum, "Pulling", configNum)
			//kv.mu.Unlock()
			//kv.intermediate_Channels[shardNum] <- configNum
			//kv.mu.Lock()
		}
	} else if updatedShard.State == Serving {
		if (previousShard.State == Serving && previousShard.Config_Num == updatedShard.Config_Num - 1) ||
		(previousShard.State == Pulling && previousShard.Config_Num == updatedShard.Config_Num) || 
		(previousShard.State == Garbage && previousShard.Config_Num == 0){
			kv.db[shardNum] = updatedShard
			log.Printf("On server %d of gid %d, successfully update shard %d to state %s at config num %d", kv.me, kv.gid, shardNum, "Serving", configNum)
			//kv.mu.Unlock()
			//kv.intermediate_Channels[shardNum] <- configNum
			//kv.mu.Lock()
		}
	} else {
		if (previousShard.State == Serving && previousShard.Config_Num == updatedShard.Config_Num - 1) {
			kv.db[shardNum] = updatedShard
			log.Printf("On server %d of gid %d, successfully update shard %d to state %s at config num %d", kv.me, kv.gid, shardNum, "Sending", configNum)
			//kv.mu.Unlock()
			//kv.intermediate_Channels[shardNum] <- configNum
			//kv.mu.Lock()
		}
	}
}

func(kv *ShardKV) applyOperation(operation Op) {
	op := operation.Operation

	if op == "Update_Config" {
		//log.Printf("On server %d of gid %d, try to update config to %d", kv.me, kv.gid, operation.New_Config.Num)
		newConfig := copyConfig(operation.New_Config)
		kv.updateConfig(newConfig)
		return
	} 
	
	if op == "Update_Shard" {
		//log.Printf("On server %d of gid %d, try to update shard %d to state %d at config num %d", kv.me, kv.gid, operation.Shard_To_Update.Shard_Num, operation.Shard_To_Update.State, operation.Shard_To_Update.Config_Num)
		updatedShard := copyShard(operation.Shard_To_Update)
		kv.updateShard(updatedShard)
		return 
	}

	Sequence_Number := operation.Sequence_Number
	Client_Serial_Number := operation.Client_Serial_Number

	key := operation.Key
	value := operation.Value

	shard := operation.Shard_Num
	client_Config_Num := operation.Client_Config_Num

	client_Info_This, ok := kv.clients_Info[Client_Serial_Number]
	if !ok {
		// means this leader is the first leader that has received request from this client
		// and This kvserver has not processed any request for this client and no other server has
		// (it others do, )
		client_To_Add := Client{}
		client_To_Add.Received_Sequence_Number = Sequence_Number - 1 //this client must have received all
		// requests before sequence number or it will not fire this request
		// but in this case, this is certainly command for op with seq_num 1
		client_To_Add.Last_Processed_Sequence_Number = default_sentinel_index // well This kvserver has
		// not execute any operation on this client yet, so we need to wait raft send command
		// so that we can eventually apply operations until the server is at least up to date 
		// as Received_Sequence_Number
		client_To_Add.Cached_Response = make(map[int]*StoredReply)
		// save all responses from Received_Sequence_Number + 1 (since we are not sure if client has received
		// previous response or not, we don't delete until future rpc indicate we can)
		kv.clients_Info[Client_Serial_Number] = &client_To_Add

		client_Info_This = kv.clients_Info[Client_Serial_Number] 

	}
	last_Processed_Sequence_Number := client_Info_This.Last_Processed_Sequence_Number
	if Sequence_Number <= last_Processed_Sequence_Number {
		// if sequence number for this op is <= seq num of last op for this client the
		// server has processed, we do not want to re peat execution
		return
	}

	if Sequence_Number != last_Processed_Sequence_Number + 1 {
		// to ensure linearizability
		// only process a request if current request's sequence number is 1 above previous op done on current client
		return
	}



	replyToStore := StoredReply{}

	/*if kv.db[shard].Config_Num != client_Config_Num {
		replyToStore.Err = ErrWrongGroup // since there is an issue with shard ownership consistency
		// we want to ensure the client gets reply from server whose config is in sync with the client server
	} else if kv.configs[client_Config_Num].Shards[shard] != kv.gid {
		// that at current config, the gid server is not the rightful owner
		replyToStore.Err = ErrWrongGroup
		// which should not be possible when client config number and server config number are same
		// but no harm is introduced so why not ...
	} else if kv.db[shard].State == Pulling {
		// that at current config, the gid server is the rightful owner but the 
		// server is still waiting for consensus on transitioning to serving state
		replyToStore.Err = ErrWrongGroup
		// initially I had ErrPulling, but it is possible that 
		// when the system retries based on expection that shard will be ready to serve,
		// the shard might have already been migrated to higher version
		// so simply reply ErrWrongGroup to force client retry request with most up-to-date config
		
	}*/
	
	if kv.db[shard].Config_Num == client_Config_Num && kv.db[shard].State == Serving {
		// ok, make it simple, serve only when system owns the shard and the config number is the same
		// are we safe if we own the shard but config number of client is not the same as config number of shard?
		// yes, but we still want to maintain a monotonic view of the system
		data := kv.db[shard].Data

		if op == "Get" {
			dbvalue, ok:= data[key]
			if ok {
				//log.Printf("This kvserver %d is caching result for Get request with key %s and serial number %d, cached value is %s", kv.me, key, Sequence_Number, dbvalue)
				replyToStore.Err = OK
				replyToStore.Value = dbvalue
			} else {
				//log.Printf("This kvserver %d is caching result for Get request with key %s and serial number %d, there is no key so return ErrNoKey", kv.me, key, Sequence_Number)
				replyToStore.Err = ErrNoKey
			}
		} else if (op == "Put") {
			//log.Printf("This kvserver %d is caching result for Put request with key %s and serial number %d, cached value is %s", kv.me, key, Sequence_Number, value)
			data[key] = value
			replyToStore.Err = OK
			replyToStore.Value = empty_string
		} else {
			dbvalue, ok:= data[key]
			if ok {
				data[key] = dbvalue + value
				//log.Printf("This kvserver %d is caching result for Append request with key %s and serial number %d, cached value is %s", kv.me, key, Sequence_Number, dbvalue + value)
			} else {
				data[key] =  value
				//log.Printf("This kvserver %d is caching result for Append request with key %s and serial number %d, cached value is %s", kv.me, key, Sequence_Number, value)
			}
			replyToStore.Err = OK
			replyToStore.Value = empty_string
		}

	} else {
		replyToStore.Err = ErrWrongGroup
	}

	kv.clients_Info[Client_Serial_Number].Cached_Response[Sequence_Number] = &replyToStore // cache the response in case of handling retry
	kv.clients_Info[Client_Serial_Number].Last_Processed_Sequence_Number = Sequence_Number	

}
// kv server changes its database state according to committed commands
// as well as handle and cache requests related to the committed commands
func (kv *ShardKV) handleRequest(applyMessage raft.ApplyMsg) {

	if applyMessage.CommandValid {
		commandIndex := applyMessage.CommandIndex
		commandTerm := applyMessage.CommandTerm
		operation := applyMessage.Command.(Op)
		if (commandIndex == kv.lastIncludedIndex + 1) {
			log.Printf("Kvserver %d of gid %d applies operation with commandIndex %d from handleRequest with LastIncludeIndex %d, LastIncludeTerm %d", kv.me, kv.gid, commandIndex, kv.lastIncludedIndex, kv.lastIncludedTerm)
			kv.lastIncludedIndex = commandIndex
			kv.lastIncludedTerm = commandTerm
			kv.applyOperation(operation)
		} else {
			log.Printf("Kvserver %d of gid %d put operation with LastIncludeIndex %d, LastIncludeTerm %d into buffer", kv.me, kv.gid, kv.lastIncludedIndex, kv.lastIncludedTerm)
			kv.operationBuffer = append(kv.operationBuffer, operation)
			kv.indexBuffer = append(kv.indexBuffer, commandIndex)
			kv.termBuffer = append(kv.termBuffer, commandTerm)
		}
		return
	} else {
		LastIncludedIndex := applyMessage.LastIncludedIndex
		LastIncludedTerm := applyMessage.LastIncludedTerm
		if (LastIncludedIndex > kv.lastIncludedIndex) {
			r := bytes.NewBuffer(applyMessage.SnapShotByte)
			d := labgob.NewDecoder(r)

			var clients_Info map[int64]*Client //map from client serial number to its state pertaining cached responses
			var db []Shard

			if d.Decode(&clients_Info) != nil ||
			   d.Decode(&db) != nil {
				//log.Printf("Kvserver %d could not read snapshot from raft. There is error in reading.", kv.me)
				return
			} else {
				//log.Printf("Kvserver %d, having LastIncludeIndex %d and lastIncludeTerm %d, reads snapshot from raft with LastIncludeIndex %d, LastIncludeTerm %d.", kv.me, kv.lastIncludedIndex, kv.lastIncludedIndex, LastIncludedIndex, LastIncludedTerm)
				kv.lastIncludedIndex = LastIncludedIndex
				kv.lastIncludedTerm = LastIncludedTerm
				kv.clients_Info = clients_Info
				kv.db = db
				
				kv.emptyOperationBuffer()
			}
		}
	}
}

func (kv *ShardKV) FetchShard(args *FetchShardArgs, reply *FetchShardReply) {
	kv.mu.Lock()
	defer kv.mu.Unlock()
	_, isLeader := kv.rf.GetState()
	if !isLeader {
		//reply.CurrentLeaderId = currentLeaderId
		reply.Err = ErrWrongLeader
		return
	}
	if args.Num_Target > kv.db[args.ShardRequested].Config_Num {
		// the version number of requester is higher than that of current server
		// meaning the server is not up-to-date and the recipient needs to wait 
		reply.Err = ErrMigrationInconsistent
		return
	}

	if (args.Num_Target == kv.db[args.ShardRequested].Config_Num && kv.db[args.ShardRequested].State == Garbage) ||
	(args.Num_Target < kv.db[args.ShardRequested].Config_Num) {
		// the shard has been collected as garbage
		// meaning the server requesting it should already have started serving it
		// the replicating receiving err garvage just need to wait for sync agreement that upgrade
		// shard to serving state
		reply.Err = ErrGarbage
		return
	} 

	reply.Data = make(map[string]string)
	for key, value := range kv.db[args.ShardRequested].Data {
		reply.Data[key] = value
	}
	reply.Err = OK
	return
}

/*func (kv *ShardKV) syncGarbageShard(shard Shard) {

	counter := 0
	for {
		kv.mu.Lock()
		if (kv.db[shard.Shard_Num].Config_Num > shard.Config_Num) ||
		(kv.db[shard.Shard_Num].Config_Num == shard.Config_Num && kv.db[shard.Shard_Num].State == Garbage) {
			kv.mu.Unlock()
			return
		} else {
			if counter == 20 {
				shardToSync := copyShard(shard)
				shardSyncOp := Op{}
				shardSyncOp.Operation = "Update_Shard"
				shardSyncOp.Shard_To_Update = shardToSync
				kv.rf.StartQuick(shardSyncOp)
				kv.mu.Unlock()

				counter = 0
			} 
			counter++
			time.Sleep(time.Duration(1) * time.Millisecond)
		}
		
	}
}*/

func (kv *ShardKV) AckShard(args *AckShardArgs, reply *AckShardReply) {

	kv.mu.Lock()
	_, isLeader := kv.rf.GetState()
	if !isLeader {
		//reply.CurrentLeaderId = currentLeaderId
		reply.Err = ErrWrongLeader
		kv.mu.Unlock()
		return
	}
	if args.Num_Target > kv.db[args.ShardAcked].Config_Num {
		// meaning the server is not up to date and the recipient needs to wait 
		// which should not be possible since ack shard is always after fetch shard
		// and fetch shard will not be executed unless server is up to date
		reply.Err = ErrMigrationInconsistent
		kv.mu.Unlock()
		return
	}

	if (args.Num_Target == kv.db[args.ShardAcked].Config_Num && kv.db[args.ShardAcked].State == Garbage) ||
	(args.Num_Target < kv.db[args.ShardAcked].Config_Num) {
		// the shard has been collected as garbage
		// meaning the server requesting it should already have started serving it
		// the replicating receiving err garvage just need to wait for sync agreement that upgrade
		// shard to serving state
		reply.Err = ErrGarbage
		kv.mu.Unlock()
		return
	} 

	if (kv.db[args.ShardAcked].Config_Num == args.Num_Target && kv.db[args.ShardAcked].State == Sending) {
		shardToGarbage := copyShard(kv.db[args.ShardAcked])
		shardToGarbage.State = Garbage
		shardToGarbage.Data = make(map[string]string)
		reply.Err = OK
		kv.mu.Unlock()

		//go kv.syncGarbageShard(shardToGarbage)

		defer kv.syncShardMessageUntilCommit(shardToGarbage)
		return
	}
	reply.Err = OK
	kv.mu.Unlock()
	return
}

func (kv *ShardKV) fetchNewConfig() {

	for {
		kv.mu.Lock()
		newConfig := kv.mck.Query(-1)

		if (newConfig.Num >= len(kv.configs)) {
			for i := len(kv.configs); i < newConfig.Num; i++ {
				configToAdd := kv.mck.Query(i)
				opToRaft := Op{}
				opToRaft.Operation = "Update_Config"
				opToRaft.New_Config = copyConfig(configToAdd)
				kv.rf.StartQuick(opToRaft)
	
				log.Printf("On server %d of gid %d, Starts agreement on config %d", kv.me, kv.gid, configToAdd.Num)
	
			}
	
			opToRaft := Op{}
			opToRaft.Operation = "Update_Config"
			opToRaft.New_Config = copyConfig(newConfig)
			kv.rf.StartQuick(opToRaft)
			log.Printf("On server %d of gid %d, Starts agreement on config %d", kv.me, kv.gid, newConfig.Num)

		}
		kv.mu.Unlock()

		time.Sleep(time.Duration(config_query_interval_millisecond) * time.Millisecond)
	}
}


//
// servers[] contains the ports of the servers in this group.
//
// me is the index of the current server in servers[].
//
// the k/v server should store snapshots through the underlying Raft
// implementation, which should call persister.SaveStateAndSnapshot() to
// atomically save the Raft state along with the snapshot.
//
// the k/v server should snapshot when Raft's saved state exceeds
// maxraftstate bytes, in order to allow Raft to garbage-collect its
// log. if maxraftstate is -1, you don't need to snapshot.
//
// gid is this group's GID, for interacting with the shardmaster.
//
// pass masters[] to shardmaster.MakeClerk() so you can send
// RPCs to the shardmaster.
//
// make_end(servername) turns a server name from a
// Config.Groups[gid][i] into a labrpc.ClientEnd on which you can
// send RPCs. You'll need this to send RPCs to other groups.
//
// look at client.go for examples of how to use masters[]
// and make_end() to send RPCs to the group owning a specific shard.
//
// StartServer() must return quickly, so it should start goroutines
// for any long-running work.
//
func StartServer(servers []*labrpc.ClientEnd, me int, persister *raft.Persister, maxraftstate int, gid int, masters []*labrpc.ClientEnd, make_end func(string) *labrpc.ClientEnd) *ShardKV {
	// call labgob.Register on structures you want
	// Go's RPC library to marshall/unmarshall.
	labgob.Register(Op{})

	kv := new(ShardKV)
	kv.me = me
	kv.maxraftstate = maxraftstate
	kv.make_end = make_end
	kv.gid = gid
	kv.masters = masters

	// Your initialization code here.

	// Use something like this to talk to the shardmaster:

	kv.mck = shardmaster.MakeClerk(kv.masters)
	
	kv.applyCh = make(chan raft.ApplyMsg)
	kv.rf = raft.MakeWithSnapshot(servers, me, persister, kv.applyCh, maxraftstate)
	 


	if maxraftstate != -1 {
		lastIncludedIndex, lastIncludedTerm, snapShotByte := kv.rf.SendSnapShotToKvServer() 
		r := bytes.NewBuffer(snapShotByte)
		d := labgob.NewDecoder(r)

		var clients_Info map[int64]*Client //map from client serial number to its state pertaining cached responses
		var db []Shard
		var configs []shardmaster.Config

		if d.Decode(&clients_Info) != nil ||
			d.Decode(&db) != nil ||
			d.Decode(&configs) != nil{
			//log.Printf("could not read snapshot from raft for This kvserver %d. There is error in reading.", kv.me)
			kv.lastIncludedIndex = default_sentinel_index
			kv.lastIncludedTerm = default_start_term
			kv.clients_Info = make(map[int64]*Client)
			kv.db = make([]Shard, shardmaster.NShards)

			for i := 0; i < shardmaster.NShards; i++ {
				db[i] = Shard{}
				db[i].Config_Num = 0
				db[i].State = Garbage
				db[i].Data = make(map[string]string)
			}

			kv.configs = make([]shardmaster.Config, 1)
			kv.configs[0].Groups = map[int][]string{}
			
		} else {
			kv.lastIncludedIndex = lastIncludedIndex
			kv.lastIncludedTerm = lastIncludedTerm
			kv.clients_Info = clients_Info
			kv.db = db

			kv.configs = configs
			kv.emptyOperationBuffer()
		}

	} else {
		kv.lastIncludedIndex = default_sentinel_index
		kv.lastIncludedTerm = default_start_term
		kv.clients_Info = make(map[int64]*Client)

		kv.db = make([]Shard, shardmaster.NShards)
		for i := 0; i < shardmaster.NShards; i++ {
			kv.db[i] = Shard{}
			kv.db[i].Config_Num = 0
			kv.db[i].State = Garbage
			kv.db[i].Data = make(map[string]string)
		}

		kv.configs = make([]shardmaster.Config, 1)
		kv.configs[0].Groups = map[int][]string{}
	}

	kv.operationBuffer = make([]Op, 0)
	kv.indexBuffer = make([]int, 0)
	kv.termBuffer = make([]int, 0)

	kv.config_Update_Channels = make([]chan int, shardmaster.NShards)
	kv.intermediate_Channels = make([]chan int, shardmaster.NShards)

	for j := 0; j < shardmaster.NShards; j++ {
		kv.config_Update_Channels[j] = make(chan int)
		//kv.intermediate_Channels[j] = make(chan int)
	}

	go kv.fetchNewConfig()	

	for k := 0; k < shardmaster.NShards; k++ {
		shardNum := k
		go kv.migrateShard(shardNum, kv.config_Update_Channels[shardNum])
	}

	go func(kv *ShardKV) {	
		
		for applyMessage := range kv.applyCh {
			kv.mu.Lock()
			log.Printf("kvserver %d Locked", kv.me)
			if kv.killed(){
				kv.mu.Unlock()
				return
			} else {
				_, isLeader := kv.rf.GetState()
				
				if isLeader {
					if kv.maxraftstate != -1 {
					
						snapShotSize := kv.rf.GetRaftStateSize()
					
						if snapShotSize >= kv.maxraftstate {
							//log.Printf("kvserver %d make snapshot in StartShardKV with LastIncludeIndex %d and LastIncludeTerm %d", kv.me, kv.lastIncludedIndex, kv.lastIncludedTerm)
							kv.tryInitSnapShot()
						}
					}
					
				}
				kv.handleRequest(applyMessage)
				log.Printf("kvserver %d finished handling request", kv.me)
				//log.Printf("kvserver %d unlocked", kv.me)
				kv.mu.Unlock()
				
			}
		}
	}(kv)

	return kv
}